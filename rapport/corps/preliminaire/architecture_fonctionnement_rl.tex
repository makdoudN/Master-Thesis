\newpage
\section{Revue de la littérature et état de l'art}

Dans cette partie, nous allons passer en revue les principales explications du fonctionnement de notre contrôleur. 
En particulier, nous expliquerons comment fonctionne l'apprentissage par renforcement, les intérêts de cette technologie et ses limites.  

Nous mettrons en relation l'apprentissage par renforcement avec l'apprentissage profond (deep learning) comme un moyen de pallier aux limitations de l'apprentissage par renforcement et ses autres bénéfices.

Enfin, nous discuterons des moyens mis en place pour tester nos algorithmes sur des cas moins complexes pour s'assurer du fonctionnement de nos codes.

\subsection{Fonctionnement de l'apprentissage par renforcement et intérêt}

L'apprentissage par renforcement est une sous partie de l'apprentissage automatique. Elle fait référence à un apprentissage qui est à partir d'expériences de façon à optimiser un récompense en cours du temps. A chaque action effectuée par l'agent, l'environnement attribut à celui ci une récompense. L'objectif est de trouver la séquence d'actions qui mène à la plus grande accumulation de récompenses. L'agent va être uniquement guider par son expérience et devrait apprendre de ses erreurs. Imaginons le cas suivant: nous devons apprendre un nouveau tour à notre chien néanmoins, il est impossible de lui exprimer clairement ce que nous souhaitons de lui. Notre seul levier est la quantité de récompenses ou au contraire de punition que va recevoir le chien. Ce problème est connu sous le nom de: \emph{problème d'assignement du crédit}. 

Une des particularités de l'apprentissage par renforcement réside dans le fait que notre politique d'actions influencent grandement les états rencontrés. Prenons l'exemple d'un agent autonome dans un labyrinthe à la jonction entre deux salles différentes. Si la séquence d'actions fait se retrouver notre agent dans une des salles, il n'aura accès qu'a la vision de cette salle.

\subsubsection{Formalisation de l'approche par renforcement}

Nous allons proposer un formalisme  permettant d'introduire les notions importantes de l'apprentissage par renforcement. Nous commençons par expliquer de façon simple les notions indispensable puis nous nous servirons des processus de décision markoviens pour avoir un cadre solide.

Nous considérerons: 

\begin{itemize}
    \item Un ensemble d'états noté $\bm{\mathbb{S}}$
    
    Cette ensemble d'états regroupe l'ensemble des états possible pouvant être fournis par l'environnement. Dans le cas du jeux d'échecs, cela correspondrait au nombre de positions possibles atteignables ($\sim 10^{43}$ soit un nombre supérieur au nombre d'atomes dans l'univers)
    
    \item Un ensemble d'actions noté $\bm{\mathbb{A}}$
    
    Cette ensemble d'action regroupe les actions possibles par l'agent en fonction de l'état dans lequel il se trouve. Il est possible d'imaginer des états pour lesquelles l'ensemble des actions soit différent. Dans le cas des échecs les actions possibles peuvent différer en fonction de l'état de la partie (exemple: clouage, roi en échec ...).

    \item Un ensemble de récompenses noté $\bm{\mathbb{R}}$
    
    Cette ensemble regroupe les récompenses que peut fournir l'environnement. C'est récompense peuvent être nul, négative ou positive (nous parlerons de récompense quelque soit la valeur de celle ci). Les récompenses dépendent à la fois de l'état mais aussi de l'action effectué par l'agent.
    
    Selon le cas, il peut être à l'expérimentateur de définir une politique de récompense mais dans de nombreux cas l'ensemble de récompense se fixe facilement. Dans le cas de notre labyrinthe l'agent reçoit +1 s'il trouve la sortie 0 sinon. Nous expliquerons plus tard pourquoi cette politique de récompense n'est pas forcement bien adapté.
    
    \item La dynamique de l'environnement $\bm{\mathbb{T}}$
    
    La dynamique de l'environnement représente la probabilité, sachant que j'effectue une action $\bm{a \in \mathbb{A}}$ et que je suis d'en un certain état $\bm{s_t \in \mathbb{S}}$, d'atterrir dans un état  $\bm{s_{t+1} \in \mathbb{S}}$. C'est une connaissance sur le modèle qui dans bien des cas nous fait défaut. En l'occurrence dans notre cas pendant ce stage, la dynamique est une donné non connue. Il est à noter que ce qui est précédemment énoncé n'est vrai que sous l'hypothèse de Markov qui nous assure que notre dynamique est conditionnellement indépendante de  des états et actions précédents. Cette hypothèse est fausse en pratique et nous discuterons des stratégies utilisées pour pallier à ce problème. 
    
    \item La politique d'action $\bm{\Pi}$
    
    La politique d'action peut être défini comme la probabilité, sachant que je suis dans un certain état $\bm{s_t \in \mathbb{S}}$, d'effectuer une action $\bm{a \in \mathbb{A(s_t)}}$.
    Dans un soucis de généralité, nous considérons une politique stochastique néanmoins la politique peut être déterministe. En apprentissage par renforcement nous cherchons à optimiser notre politique d'action dans le but de maximiser la quantité de récompense.
        

\end{itemize}

Ainsi un problème d'apprentissage par renforcement profond classique suit le schéma suivant. A chaque pas de temps t, l'agent va percevoir un état $\bm{s_t \in \mathbb{S}}$. L'agent va choisir l'action à effectuer $\bm{a \in A(s_t)}$ et va recevoir une récompense $\bm{r_{t+1}}$. Notre objectif va donc être de maximiser la somme pondérée des récompenses obtenues par notre séquence d'actions. 


On définira notre problème sous la forme d'un processus décisionnel de Markov, soit un 5-tuple $\bm{\big(\mathbb{S}, \mathbb{A}, \mathbb{T}, \mathbb{R}, \gamma \big)}$. Le paramètre $\gamma$ qui intervient dans la définition de notre problème est important car c'est lui qui va pondérer l'importance d'un récompense au cours du temps. Il existe de multitudes choix pour la spécification de ce paramètre, néanmois dans notre cas $\gamma$ sera égale à .99. Cela pour deux raisons, la première est que nous souhaitons que les récompenses tardives soient prisent en compte mais de façon plus faible que celle qui sont immédiates. Deuxièmement, pour des raisons de stabilité, il est important théoriquement que le paramètre $\gamma$ soit inférieur à 1.


Notre objectif est de trouver la politique $\bm{\Pi}$ tel que la somme pondérée des récompenses (notée $\bm{G_t}$) soit maximale avec:

\begin{align}
G_t = \discountedReward \quad \text{sachant} \quad a_t = \Pi(s_t)
\end{align}

Pour déterminer la politique optimal, il est utile de définir la fonction de valeur d'états $\bm{V(s_t)}$ qui représente le gain moyen à partir d'un certain état en suivant une  politique $\bm{\Pi}$ et la fonction de valeur d'états actions $\bm{Q(s_t, a_t)}$ qui représente le gain moyen à partir d'un certain état et en effectuant une certaine action puis suivant une  politique $\bm{\Pi}$.
\begin{gather}
\quad V(s_t) = \valueFunction \\[.1cm]
\quad  Q(s_t,a_t) = \QFunction
\end{gather}


Une propriété fondamentale des fonctions d'état et d'état-action est qu'ils respectent une relation de récursivité bien connu: l'équation de Bellman (pour $V$ et $Q$):

\begin{gather}
V_\pi(s_t) = \underset{a}{\bsum}\: \policyc{blue} \:\underset{s', r}{\bsum}\:\dynamicsc{red}\: \bigg[\: r + \gamma V_\pi(s') \:\bigg] \\
Q_\pi(s_t, a_t) = \underset{s', r}{\bsum}\:\dynamicsc{red}\: \bigg[\: r + \gamma V_\pi(s') \:\bigg] 
\end{gather}

\bigskip

les relations précédentes sont particulièrement intéressantes car elles nous permettent d'exprimer la valeur d'un état à partir de la valeur des états successifs à celui ci. Ces deux relations sont à l'origine de nombreuses méthodes en apprentissage par renforcement. Notamment, elles seront à l'origine des méthodes appliquées pendant ce stage dans le contrôle d'un agent dans la simulation de Thales SE-STAR.

Pourtant, nous n'avons toujours pas explicité la forme de la politique optimale. Nous allons nous aider des fonction d'état, et d'état-action qui fournisse un indicateur sur la pertinence de la politique choisie. Ainsi, on pourra dire qu'une politique $\pi$ est meilleurs qu'une politique $\mu$ si $V_\pi(s) > V_\mu(s), \forall s $. Il parait donc intéressant de rechercher la politique maximisant la fonction d'état $ V_* = \underset{\pi}{\max} \:V_\pi(s) $. On peut faire la même chose avec la fonction d'état-action: $ Q_* = \underset{\pi}{\max} \:Q_\pi(s, a) $.

Nous pouvons explicité $Q^*$ et $V^*$ correspondant aux équations optimales de Bellman respectives:

\begin{gather}
V^*(s) = \underset{a}{\max} \: \underset{s', r}{\bsum} \: p(s',r \vert s, a) \bigg[\: r + \gamma V_\pi(s') \:\bigg]
\\
Q^*(s, a) = \underset{s', r}{\bsum} \: p(s',r \vert s, a) \bigg[\: r + \gamma \: \underset{a'}{\max} \: Q_\pi(s', a') \:\bigg]
\end{gather}
\bigskip


A partir des fonctions état et état-action, il est relativement aisé de déterminer la politique optimale. Une fois qu'on a déterminé $V^*$ nous pouvons déterminer l'action optimal en faisant une recherche à un pas dans le temps. Ce qui est remarquable est qu'en apprentissage par renforcement, il y a un compromis fondamental entre un objectif à court et long terme pourtant nous venons d'affirmer qu'en exploitant $V^*$ et en faisant notre recherche à un pas nous avons pris compte des effets à long termes. Avec la fonction d'état-action c'est encore plus simple car il n'y a plus besoin de recherche auxiliaire car la fonction d'état-action encode déjà cette recherche, il suffit donc d'exploiter la fonction d'état-action optimale à chaque action pour obtenir la politique optimal: $\pi^*(s) = \underset{a}{\max}\:Q^*(s,a)$ 

Malheureusement, il est, dans la plupart des cas, impossible de déterminer les fonctions d'états ou d'états-actions optimales car cela reviendrait à une recherche exhaustive de toutes les possibilités avec la connaissance de leurs probabilités d'apparaître et la connaissance du gain espéré associés. Néanmoins, les fonctions d'état et d'état-action sont utiles à l'apprentissage et au contrôle de notre agent. Dans la partie suivante nous allons expliquer comment approximer ces fonctions pour pouvoir commencer à établir des algorithmes de contrôle efficace.

En résumé, l'apprentissage par renforcement propose de contrôler un agent grâce à des stimulis (récompenses) données par l'environnement ou l'expérimentateur. 

L'objectif est de déterminer une politique (soit un fonction $\mathbb{S} \: \rightarrow \: \mathbb{A}$) qui maximise le gain cumulé pondéré (pondération utile pour favoriser l'importance des récompenses à court termes tout en prenant en compte les récompenses à long termes).


Pour déterminer la politique optimale, nous nous aidons de deux fonctions (les fonctions d'états et d'états-actions) qui sont une mesure de la quantité de récompenses pouvant être espéré à un certain état (ou à un état en effectuant une certaine action) en suivant une politique. 

On peut dès lors en trouvant une fonction d'états (ou d'états-action) optimales dérivé une politique optimale. 

Sauf cas jouet, les fonctions d'états (états-actions) sont inconnus, notre objectif va être de trouver une façon de les approximer pour permettre l'établissement d'algorithme de contrôle par apprentissage par renforcement.


Enfin, nous montrerons que nous ne sommes pas obliger de passer par ce intermédiare pour déterminer la politique optimale tout en insistant sur l'importance de la connaissance des fonctions d'états est un atout quelque soit l'approche utilisé.
