\subsubsection{Introduction des principales approches en apprentissage par renforcement}

Après avoir introduit les principaux concepts de l'apprentissage par renforcement, il convient d'en extraire des algorithmes permettant de trouver notre politique optimale. Un difficulté majeure est que nous n'avons ni connaissance des fonctions d'états (et d'états-actions) ni connaissance de la dynamique du modèle (noté $\textcal{T}$).

La question est donc de savoir s'il est possible d'approximer ces fonctions, ou alors de s'en passer.

Avant de répondre à ces questions, il est important de faire la différence entre deux types d'approches en apprentissage par renforcement: 

\begin{enumerate}
    \item \textbf{Model Free} (Apprentissage sans modèle): \\
        Dans ce type d'approche, nous n'utilisons pas un modèle de l'environnement pour déterminer la politique optimale, uniquement une recherche basée sur notre expérience. Intuitivement, nous pourrions penser cette approche inférieure à l'approche \emph{Model Based} pourtant nous verrons que l'approche \emph{Model Free} est plus simple à mettre en place et est plus performante dans la plupart des cas que nous avons rencontrés. Cela s'explique par la difficulté d'apprentissage de la dynamique de l'environnement (qui dans bien est cas n'est pas stationnaire).
    
    \item \textbf{Model Based} (Apprentissage avec modèle): \\
        Dans ce type d'approche, nous allons utiliser la connaissance du modèle (ou l'approximation interne du modèle par l'agent) pour déterminer la politique optimale. L'apprentissage avec modèle apparaît comme plus puissant mais est encore un problème non résolu (tout comme en Model Free mais des algorithmes ont déjà montré leur capacité à résoudre des environnements difficiles). La difficulté est que, bien souvent, l'entrée n'est pas très informative sur la dynamique de l'environnement. Ainsi, une image 2D représentant un monde 3D est une entrée dont il est difficile de tirer un modèle. 
\end{enumerate}

Dans cette partie, nous allons nous concentrer sur l'établissement d'algorithmes \emph{Model Free}. 


Précédemment, nous avons établi que nous n'avions aucune connaissances des fonctions d'états, ni de la dynamique de l'environnement. Notre objectif est de trouver une approximation des fonctions optimales d'états qui nous permettrait d'établir une politique optimale. Pour ce faire nous devons trouver un moyen d'approximer la fonction d'état et de trouver un moyen d'établir à quel point notre approximation est loin de la fonction optimale.

Nous expliquerons plus tard le mécanisme d'approximation de fonctions utilisé (réseau de neurones). Pour cette partie, nous considérerons juste la fonction $\omega, s \rightarrow \overset{\sim}{V_\omega}(s)$ qui associe une matrice $\omega$ et un état à une approximation de la fonction d'état. 

Nous pouvons ainsi formaliser, en un problème d'optimisation, notre objectif:

$$ \omega^* = \underset{\omega}{\min} \norm{{V^\pi - \overset{\sim}{V}_\omega}}_2$$

Nous pouvons donc, en utilisant une méthode d'optimisation telle que la descente de gradient stochastique, déterminer un minimum local:

\begin{equation}
\begin{aligned}
\omega_{t+1} = \omega_t - \tfrac{1}{2} \: \alpha \nabla_\omega \norm{{V^\pi - \overset{\sim}{V}_\omega}}_2 \\[0.75ex]
\omega_{t+1} = \omega_t + \tfrac{1}{2} \: \alpha (V^\pi - \overset{\sim}{V}_\omega)\nabla_\omega \overset{\sim}{V}_\omega

\end{aligned}
\end{equation}

Cela nous pose un problème de taille car nous n'avons pas connaissance de la fonction d'état. Or, notre précédante équation repose sur la connaissance de celle-ci et il ne serait pas pertinant de chercher une approximation d'une fonction déjà connue. Il existe plusieurs méthodes pour l'approximer, nous allons en donner deux. Une méthode basée sur la méthode de Monte Carlo, et l'autre basée sur du \emph{bootstrapping} (le bootstrapping étant basé sur des processus itératifs convergeant vers la valeur qui est souhaitée)

\bigskip 

\begin{center}

\setlength{\tabcolsep}{10pt}
\renewcommand{\arraystretch}{1.5}
 
\begin{tabular}{ |c|c|  }
\hline
\multicolumn{2}{|c|}{Approximations possibles de la fonction d'état} \\
\hline
Monte-Carlo &  $\overset{\sim}{v}(s_t) = \mathbb{E}_\pi \big[G_t \vert S_t = s\big] \sim \frac{1}{N}\:\underset{k=0}{\sum}^N\: \gamma r_{t+k}$
\\[1.5ex]
\hline
Bootstrapping & $v_{k+1} =  \underset{a}{\sum}\: \pi(a, s)\: \underset{s', r}{\sum} \:p(s', r \:\vert\: s, a) \: \big[ r + \gamma\: v_k(s')\:\big]$  \\

\hline
\end{tabular}
\end{center}

\bigskip 

Nous commencerons par envisager l'approche par bootstrapping. La première remarque est que le bootstrapping requiert la connaissance du modèle de l'environnement ce qui est particulièrement handicapant dans notre cas. Néanmoins, nous avons des garantis sur la convergence de notre approximation vers la fonction d'état. Ainsi, si nous sommes en capacité de connaître le modèle, il nous suffit d'itérer pour connaître la véritable fonction d'état. Pourtant, le véritable problème que nous avons avec cette méthode est un problème qui apparaîtra souvent et qui est dénommé \emph{la malédiction de la dimensionalité}. Plus la dimensionnalité de l'état est grande plus le coût de cette approche augmente (de façon exponentielle). Dans notre cas d'usage, l'état sera un tableau de pixels ce qui rend impossible l'utilisation de ce genre d'algorithmes.

Si l'approche précédente reste inapplicable, la philosophie du bootstrapping n'a pas à être jetée. Il n'est pas pertinent de rechercher à tendre vers la fonction d'état en itérant selon la formule de bellman. Cependant il sera intéressant de chercher à itérer d'une autre manière de façon à s'approcher de la véritable fonction d'état. Toute la question est de trouver une formule qui permet d'itérer (à faible coût) et qui garantie de tendre vers une approximation de la fonction d'état pertinente.


Passons maintenant à la méthode via \emph{Monte Carlo}. Cette méthode s'intègre parfaitement dans l'esprit de l'apprentissage par renforcement. En effet, un agent va jouer un épisode et récupérer un ensemble d'actions, états et récompenses. A partir de cet ensemble, nous sommes en capacité d'approximer la fonction d'état tel que l'approximation de la fonction d'état pour un état s correspond aux récompenses obtenues dès la première visite de l'état s (\emph{First visite Monte Carlo}). Dans le cas où nous allons jouer plusieurs épisodes, nous pourrons affiner notre approximation en approchant notre fonction d'état pour un état s comme étant la moyenne des récompenses obtenues dès la première visite de l'état s sur l'ensemble des épisodes joués. 
Les méthodes d'approximation de la fonction d'état possèdent quelques avantages par rapport au bootstrapping:

\begin{itemize}
\item Insensible à la dimension de l'état ce qui est intéressant dans notre cas.
\item Nous pouvons générer des épisodes en commençant par les états nous intéressant pour estimer leur fonction d'état ce qui n'est pas possible.
\end{itemize}

Néanmoins, l'approche par Monte Carlo possède aussi des désavantages tels que le fait d'être obligé de jouer un épisode entier pour pouvoir estimer la fonction d'état. Cela pourra être problématique dans le cas où l'épisode est  long . De plus, il parait pertinent de pouvoir itérer à partir d'estimations antérieures ce qui n'est pas le cas avec les méthodes de Monte Carlo. 

\subsubsection{TD Learning: la base des algorithmes pour le contrôleur}
En se basant sur l'approximation décrite précédemment en utilisant l'approche de Monte-Carlo, pour un environnement non stationnaire (qui change en fonction du temps), nous pouvons trouver une formule approximant la fonction d'état:
 
 \begin{equation}
\overset{\sim}{V}_{\pi}(S_t) \leftarrow \overset{\sim}{V}_{\pi}(S_t) + \alpha \bigg(\textcolor{blue}{G_t} - \overset{\sim}{V}_{\pi}(S_t)\bigg)
\end{equation}
 
Le problème de la formule ci-dessus est que, pour mettre à jour notre croyance sur la fonction d'état, nous sommes obligé d'attendre la fin d'un épisode.

Nous allons voir dans cette partie qu'il est possible de combiner les approches de Monte-Carlo et Bootstrapping pour garder les avantages dès deux méthodes tout en gommant leurs défauts. Pour cela, rappelons la formule de la fonction d'état: 

\begin{align*}
V_{\pi} = \valueFunction \\
= \mathbb{E}_\pi \big[\textcolor{blue}{G_t} \vert S_t = s\big] \\
= \mathbb{E}_\pi \big[\textcolor{red}{r_{t+1} + \gamma V_{\pi}(S_{t+1})} \vert S_t = s\big]
\end{align*}
 
Nous pouvons en déduire une nouvelle formule pour estimer la fonction d'état:
 
\begin{equation}
\overset{\sim}{V}_{\pi}(S_t) \leftarrow \overset{\sim}{V}_{\pi}(S_t) + \alpha \bigg(\textcolor{red}{r_{t+1} + \gamma V_{\pi}(S_{t+1})} - \overset{\sim}{V}_{\pi}(S_t)\bigg)
\end{equation}

Nous obtenons ainsi une formule se basant sur du bootstrapping car nous nous servons de la fonction d'état à $S_{t+1}$ et nous n'avons toujours pas besoin de la connaissance du modèle (Monte-Carlo).

La formule précédemment constitue la base sur laquelle nous allons nous appuyer pour créer notre contrôleur. Néanmoins, si la fonction d'état est importante, il est plus intéressant d'approximer la fonction d'état action dans le contexte d'un contrôle.

Pour le contrôle, c'est donc la formule ci-dessous, qui se nomme SARSA (pour \emph{state action reward state action} \footnote{voir la formule ci-dessous pour comprendre pourquoi}, qui sera utilisée:

\begin{equation}
\overset{\sim}{Q}_{\pi}(S_t, A_t) \leftarrow \overset{\sim}{Q}_{\pi}(S_t, A_t) + \alpha \bigg(r_{t+1} + \gamma \overset{\sim}{Q}_{\pi}(S_{t+1}, A_{t+1}) - \overset{\sim}{Q}_{\pi}(S_t, A_t)\bigg)
\end{equation}

Il existe dans la littérature une pléthore de variations de SARSA en particulier, une des variations qui va nous intéresser est celle qui s'appelle Q-Learning \cite{Watkins92q-learning}: 

\begin{equation}
\overset{\sim}{Q}_{\pi}(S_t, A_t) \leftarrow \overset{\sim}{Q}_{\pi}(S_t, A_t) + \alpha \bigg(r_{t+1} + \gamma \underset{a'}{\max}\overset{\sim}{Q}_{\pi}(S_{t+1}, a') - \overset{\sim}{Q}_{\pi}(S_t, A_t)\bigg)
\end{equation}

Malgré leur relative proximité, nous privilégierons le Q-Learning pour dès raison que nous expliquerons plus tard. 

Nous allons finir cette partie sur le TD Learning par expliquer la relation entre l'approximation de la fonction d'état par optimisation d'une fonction de perte et le Q learning. Cette introduction est importante car c'est l'algorithme le plus simple qui a été utilisé durant ce stage sur différents environnements. Nous allons expliquer comment fonctionne le Deep Q Learning \cite{2013arXiv1312.5602M} \cite{mnih-dqn-2015} (soit le premier algorithme d'apprentissage par renforcement profond). 

Néanmoins, nous n'expliquerons pas totalement cet algorithme car il requiert quelques connaissances en deep learning pour la compréhension du mécanisme d'approximation de la fonction. Pourtant, il est important de le mentionner maintenant car il découle du TD Learning et en particulier de l'algorithme du Q-Learning et qu'il est fondamental pour la compréhension des autres algorithmes utilisés durant ce stage.

\subsubsection{Du TD Learning au Deep Q Learning}

Un problème en apprentissage par renforcement est la représentation de la fonction d'état (d'état action). Naivement, nous pourrions imaginer un dictionnaire qui, à chaque état, garderait en mémoire la fonction d'état action pour chacune des actions. Néanmoins, dans le cas d'états représentés par des pixels, il faudrait une capacité astronomique de mémoire. 

Nous préférons donc utiliser des fonctions paramétrées par un ensemble de poids (noté $\theta$) tel que $f_\theta = \overset{\sim}{Q} \sim Q$. 

Précédemment, nous avions introduit une méthode par minimisation d'une fonction de perte pour permettre de trouver l'ensemble $\theta$ tel que $f_\theta = \overset{\sim}{Q} \sim Q$. 

Imaginons maintenant que notre agent joue dans l'environnement et récolte des transitions (un ensemble composé de l'état courant, de l'état précédent, de l'action jouée et de la récompense obtenu que l'on notera $e_i$ pour la ième transition). L'agent stockera ces transitions dans une mémoire que l'on notera $\mathcal{D}$. Soit $\mathcal{D} = \big(e_0, ..., e_N\big)$ 

La fonction de perte utilisée est: 
\begin{align}
L_i(\theta_i) = \mathbb{E_{\text{s}, \text{a} \sim \rho(.)}}\bigg[ \big( y_i - f_\theta(s, a; \theta)\big)^2 \bigg]\\
y_i = \mathbb{E}_{\text{s'} \sim \Epsilon}\big[r + \underset{a'}{\max}f_{\theta_{i-1}}(s', a') \big] 
\end{align}

Avec $\rho(s, a)$ la distribution sur la séquence d'états et d'actions.

\subsubsection{Architecture de base A2C/A3C utilisée pour le contrôle}
Après avoir représenté le \emph{Deep-Q-Learning}, nous allons introduire une architecture classique en apprentissage par renforcement profond qui se dénome A3C\cite{DBLP:journals/corr/MnihBMGLHSK16}\footnote{Asynchronous Methods for Deep Reinforcement Learning, dans la suite de ce rapport nous utiliserons le terme A3C}. 
Pour arriver à expliquer le fonctionnement de l'A3C, il convient de partir du constat qu'il serait aisé de trouver la politique optimale (paramétrisée par un ensemble de poids $\omega$) en maximisant une fonction de performance (notée $\eta$). Formellement cela reviendrais à mettre à jour les poids $\omega$ selon ;a formule suivante:

\begin{equation}
    \omega_{t+1} = \omega_t + \alpha \nabla \eta(\omega_t)
\end{equation}

Si l'approche présentée  est naturelle, elle reste néanmoins incomplète car nous n'avons pas encore explicité notre fonction de performance. Dans notre cas, la fonction de performance sera la fonction d'état $v_{\pi_\omega}$(qui donne un estimé de la récompense obtenue en étant à un certain état et en utilisant une certaine politique). Il est facile de voir que plus la politique est bonne plus la fonction d'état (pour un état $s$) est bonne. Cela correspond donc à une bonne métrique de la qualité de la politque.


Le problème réside maintenant dans notre capacité à expliciter $\nabla\eta(\omega_t)$. Ce problème a été résolue dans notre définition de $\eta$. En utilisant la méthode de Monte-Carlo REINFORCE \cite{Williams1992} (qui se base sur le théorème du gradient de la politque \cite{policygradient} qui explicite le gradient de la fonction d'état selon les poids $\omega$) 

On obtient alors:

\begin{equation}
    \omega_{t+1} = \omega_t + \alpha \gamma^t G_t \nabla \log\big(\pi(A_t \,\vert\, S_t, \omega)\big)
\end{equation}

Un dès default de la méthode REINFORCE\cite{Williams1992} est que l'estimateur du gradient qu'elle génére est hautement bruité. Pour pallier à cela, plusieurs méthodes ont été mise au points.
La façon la plus simple de réduire la variance du gradient estimé, on utilise la formule suivante qui utilise la fonction d'état:

\begin{equation}
    \omega_{t+1} = \omega_t + \alpha \gamma^t (G_t - \hat{v}(S_t, \theta_t))\nabla \log\big(\pi(A_t \,\vert\, S_t, \omega)\big)\footnote{La mise à jour de $\theta$ pour la fonction d'état a été omise pour la clarté}
\end{equation}


Comme nous l'avons précédemment décrit, les méthodes issuent de la famille de Monte-Carlo sont connues pour être lente à converger (malgré l'introduction de notre stratégie pour réduire la variance de l'estimateur du gradient). Pour accélerer la vitesse de convergence, nous allons donc nous servir du TD Learning pour introduire une nouvelle stratégie qui se base sur le gradient de la politique (comme pour REINFORCE) et qui utilise à la place du gain cumulé, une mise à jour se basant sur celle du TD-Learning. Soit:

\[
    G_t \longleftrightarrow r_t + \gamma^t \hat{v}(S_{t+1}, \theta_t)
\]

Donnant l'algorithme Actor Advantage Critic (A2C)\footnote{Advantage car $r_t$: est un estimateur de la fonction d'état-action(Q), on utilise donc la fonction d'avantage($A=Q-V$)}


\begin{equation}
    \omega_{t+1} = \omega_t + \alpha \gamma^t (r_t + \hat{v}(S_{t+1}, \theta_t) - \hat{v}(S_{t}, \theta_t)) \nabla \log\big(\pi(A_t \,\vert\, S_t, \omega)\big)
\end{equation}

\subsubsection{de l'A2C à A3C}

l'asynchronous advantage actor-critic (A3C) augmente A2C en utilisant n agents en parallèles (et n environnements) qui apprennent de manière asynchrone. Le réseau est copié et donné à chaque agent ce qui permet de n'avoir qu'un seul réseau et non n différents. A chaque mise à jour, les gradients sont envoyés au réseau principales qui à chaque nouvel épisode se copie pour les n agents. 

L'avantage de cette méthode est donc d'accéléré l'apprentissage de l'agent (en utlisant les n agents) mais le principal avantage de cette méthode est la décorrellation des gradients car les gradients sont issues des n agents. Cette problèmatique est la même que celle pour le deep-q-learning (DQN), contrairement à la mémoire utilisée dans le DQN qui utilise une partie des ressources de l'ordinateur, s'il est possible de générer n environnements en parallèle alors cette dernière méthode permet de sauvegarder des ressources. Néanmois, avec cette méthode sans module, il n'y a pas de réutilisation des résultats possibles, cela implique qu'il faudra un grand nombre d'episodes pour stabiliser un apprentissage. Un autre avantage qui vient des n agents qui apprennent en parallèle est que cela encourage l'exploration de l'environnement car les agents auront un comportement différents les uns des autres. 

L'A3C sera donc l'algorithme de base pour le contrôle d'un agent dans SE-STAR. Il sera par la suite augmenté de plusieurs modules décrits au cours de ce rapport.
